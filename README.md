# ğŸš€ A# ğŸš€ AdventureWorks Performance Dashboard

This repository showcases a full-scale data Engineer  project using the **AdventureWorks dataset**, built entirely on the Azure data platform. The goal is to create an end-to-end pipeline for data ingestion, transformation, storage, and visualization using enterprise-grade tools.

---

## ğŸ¯ Project Objective

To design a cloud-based performance dashboard using the AdventureWorks dataset, highlighting sales, product trends, customer behavior, and operational insights. The project leverages Azure-native services for a complete modern data platform implementation.

---

## ğŸ§° Tools & Technologies Used

- **Azure Data Factory** â€“ For orchestrating ETL workflows  
- **Azure Data Lake Storage Gen2** â€“ Centralized data storage (Bronze, Silver, Gold layers)  
- **Azure Databricks** â€“ Data transformation using PySpark notebooks  
- **Azure Synapse Analytics** â€“ Querying and modeling structured data for reporting  
- **Power BI** â€“ Interactive dashboard and performance reporting  
- **AdventureWorks Dataset** â€“ Source business data (Excel/CSV)

---

## ğŸ“Š Key Features

- ğŸ“¥ Automated data ingestion from source to Data Lake  
- ğŸ”„ Data cleansing and transformation in Databricks (PySpark)  
- ğŸ§® Dimensional modeling and data marts in Synapse SQL  
- ğŸ“ˆ Power BI reports showing sales, profit, and customer KPIs  
- ğŸ›ï¸ Dynamic filters and slicers for interactivity  

---

## ğŸ“¸ Sample Screenshots

### ğŸ”¹ Sales Overview  
![Sales Overview](Screenshots/sales-overview.png)

### ğŸ”¹ Profit by Region  
![Profit by Region](Screenshots/profit-by-region.png)

### ğŸ”¹ Product Performance  
![Product Performance](Screenshots/product-performance.png)

---

## ğŸ“ Folder Structure



---

## ğŸ¯ Project Objective

To build a scalable, cloud-based data pipeline that ingests raw data, processes it efficiently, and visualizes business-critical insights for decision-making. This project showcases the integration of various Azure services to handle ETL and reporting tasks.

---

## ğŸ§° Tools & Technologies Used

- **Azure Data Factory** â€“ For orchestrating data movement and transformation pipelines  
- **Azure Data Lake Storage Gen2** â€“ As a centralized data repository (Bronze/Silver/Gold layers)  
- **Azure Databricks** â€“ For data wrangling, cleansing, and applying business logic (PySpark)  
- **Azure Synapse Analytics** â€“ For data integration and performance-tuned queries  
- **Power BI** â€“ For interactive reporting and business intelligence  
- **AdventureWorks Dataset** â€“ Source dataset in Excel format  

---

## ğŸ“Š Key Features

- Automated data ingestion from source to Data Lake  
- Multi-stage data transformation using PySpark notebooks in Databricks  
- ETL pipeline orchestration using ADF  
- Data aggregation and modeling in Synapse SQL  
- Dynamic, filterable reports and dashboards in Power BI  

---




